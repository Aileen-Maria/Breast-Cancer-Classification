{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e768a305-dff8-4f73-aaec-b966a655f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Extract the features and target variable\n",
    "X = data.data  \n",
    "y = data.target  \n",
    "\n",
    "# Create a DataFrame for easier inspection\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62add61d-93b6-414a-b0df-82cd35618a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Feature scaling: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "929c751f-839a-4b26-a7e5-1bfdd974bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.12348985, -0.29680142, -0.17050713, -0.20861569, -1.2016799 ,\n",
       "         -0.7731696 , -0.76231194, -0.93324109, -1.22994935, -0.94816603,\n",
       "         -0.53359339, -0.86028757, -0.61678096, -0.39177533, -1.35556152,\n",
       "         -0.52503193, -0.4817033 , -0.97940018, -0.88459317, -0.68548672,\n",
       "         -0.19761978, -0.5067476 , -0.30791001, -0.27357592, -1.50742388,\n",
       "         -0.44926047, -0.57223884, -0.84082156, -0.8563616 , -0.76574773],\n",
       "        [-0.22826757, -0.65795149, -0.25377521, -0.2965028 , -1.80463697,\n",
       "         -0.58761605, -0.09198533, -0.54268359, -1.41998468, -0.61249143,\n",
       "         -0.83040055, -0.12266723, -0.78254381, -0.53126109, -0.36490698,\n",
       "          0.40861926,  0.57668457, -0.2482875 , -1.03572382,  0.10768859,\n",
       "         -0.42291745, -0.45849468, -0.4652873 , -0.43812681, -1.27301714,\n",
       "          0.02704209,  0.31804488, -0.37706655, -1.3415819 , -0.41480748],\n",
       "        [ 0.14553402, -1.23056444,  0.24583328, -0.01024193,  0.5191843 ,\n",
       "          1.57000613,  0.73231958,  0.38658307,  1.05420084,  1.57422827,\n",
       "          0.48747836,  0.59258929,  0.90918448,  0.18132474,  0.93956737,\n",
       "          1.50696696,  0.68362272,  0.62223771,  0.76910084,  0.62438565,\n",
       "          0.03602226, -1.1922718 ,  0.20386884, -0.12744491, -0.02487735,\n",
       "          0.77080169,  0.27261182, -0.04762652, -0.08997059,  0.4882635 ],\n",
       "        [-0.35853176, -0.67220742, -0.40093712, -0.40001429, -1.20386189,\n",
       "         -0.9706502 , -0.63470419, -0.6549921 ,  0.0965718 , -0.82798624,\n",
       "         -0.72594925, -0.5687868 , -0.65466961, -0.50893518, -0.56458126,\n",
       "         -0.52781397, -0.30147421, -0.56308997, -0.23011004, -0.61901108,\n",
       "         -0.50218886, -0.58328671, -0.50099984, -0.49338644, -0.95989496,\n",
       "         -0.66349557, -0.47014208, -0.49351467,  0.22654729, -0.80289938],\n",
       "        [-0.15747182,  0.96722386, -0.20884342, -0.24153848, -0.25469546,\n",
       "         -0.7006297 , -0.75034872, -0.63746879, -0.51824839, -0.64288172,\n",
       "         -0.20265859,  0.25520414, -0.38329214, -0.1916614 , -0.38130535,\n",
       "         -0.24293351, -0.4014164 , -0.45965487,  0.06215914, -0.45622129,\n",
       "         -0.19553369,  0.59641391, -0.29610671, -0.26673426, -0.44237358,\n",
       "         -0.65608493, -0.83513799, -0.65980195, -0.38720819, -0.80061313]]),\n",
       " array([[-4.83132293e-01, -1.32858288e-01, -4.60296543e-01,\n",
       "         -4.96271601e-01,  2.68255241e-01,  4.88189449e-02,\n",
       "         -1.03176739e-01, -2.71603231e-01,  4.54285373e-01,\n",
       "          1.47265889e-01, -2.99519903e-02, -3.07747089e-01,\n",
       "         -1.79167032e-01, -2.11119768e-01, -3.24380431e-02,\n",
       "         -3.47538054e-01, -1.74880757e-01, -2.49893631e-01,\n",
       "         -3.18790011e-01, -7.73856494e-02, -2.74805092e-01,\n",
       "         -1.44018778e-01, -3.42411953e-01, -3.57781068e-01,\n",
       "          4.70899433e-01, -7.60332839e-02, -7.13329260e-03,\n",
       "         -1.93382575e-01,  2.10480393e-01,  2.26487228e-01],\n",
       "        [ 1.34906186e+00,  5.11034285e-01,  1.29204314e+00,\n",
       "          1.31197082e+00, -4.27072812e-01, -7.88340504e-03,\n",
       "          2.56363249e-01,  8.24931429e-01, -8.23795402e-01,\n",
       "         -1.11254989e+00,  1.32377819e+00, -7.82978173e-01,\n",
       "          1.23644771e+00,  1.13580879e+00, -8.39173598e-01,\n",
       "         -4.91647505e-01, -3.18797521e-01,  2.84949345e-01,\n",
       "         -8.13399399e-01, -7.90488027e-01,  1.78833753e+00,\n",
       "          1.78776597e-01,  1.77158878e+00,  1.72647171e+00,\n",
       "         -5.42398723e-01, -1.04328486e-01,  1.03444793e-03,\n",
       "          1.01968394e+00, -5.33416957e-01, -1.00866239e+00],\n",
       "        [ 3.63584944e-01,  7.62285993e-02,  3.89285221e-01,\n",
       "          2.48676288e-01,  9.62855964e-01,  3.71435764e-01,\n",
       "          7.52901480e-01,  8.61040076e-01,  4.76642471e-01,\n",
       "         -6.49788609e-01,  2.39621985e-01, -8.05341989e-01,\n",
       "          1.03577030e-01,  1.57974705e-01, -2.61693703e-01,\n",
       "         -5.85123905e-01, -1.37569003e-01, -1.59950068e-01,\n",
       "         -7.99660249e-01, -5.02300996e-01,  6.20127351e-01,\n",
       "          8.22707633e-02,  5.30729365e-01,  4.80937264e-01,\n",
       "          9.92769728e-01, -6.52541593e-02,  5.64608545e-01,\n",
       "          5.88684856e-01, -7.39036912e-02, -1.91325994e-01],\n",
       "        [-5.02955104e-01, -3.51449124e-01, -4.44632250e-01,\n",
       "         -5.35611545e-01,  6.86470336e-01,  5.53274335e-01,\n",
       "         -1.37136866e-01, -5.42949096e-01,  2.95005093e-02,\n",
       "          1.15428968e+00, -7.86275906e-01,  4.94265614e-01,\n",
       "         -3.17934218e-01, -5.15489572e-01,  9.47284251e-01,\n",
       "          4.22529435e-01,  5.44703063e-01, -4.10960730e-02,\n",
       "          1.87060500e-01,  8.36276772e-01, -7.10797821e-01,\n",
       "         -4.31872385e-01, -5.37317676e-01, -6.42148861e-01,\n",
       "          5.75273492e-01,  9.30642331e-02, -1.43942947e-01,\n",
       "         -6.29083469e-01, -5.25383509e-01,  5.74569652e-01],\n",
       "        [-7.46492503e-01, -1.12126903e+00, -7.25765081e-01,\n",
       "         -7.16686894e-01,  2.82074523e-01,  1.70044659e-01,\n",
       "         -2.66288261e-01, -5.97377572e-01,  5.55837904e-02,\n",
       "          7.12249060e-01, -4.35691848e-01,  1.08806348e+00,\n",
       "         -5.90732510e-01, -4.04269650e-01,  1.63858813e+00,\n",
       "          8.77114073e-01,  7.75236398e-01,  4.84110093e-01,\n",
       "         -2.46347222e-01,  6.50069424e-01, -8.40135377e-01,\n",
       "         -9.69310044e-01, -8.65086150e-01, -7.43896745e-01,\n",
       "          1.18636984e-01, -2.51194058e-01, -4.53296119e-01,\n",
       "         -6.99924046e-01, -8.90102080e-01, -1.25596144e-01]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the first few rows of the scaled training and test data\n",
    "X_train_scaled[:5], X_test_scaled[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29175453-808f-44fa-9289-56f0e2f684ec",
   "metadata": {},
   "source": [
    "### Preprocessing Steps and Justification\n",
    "\n",
    "#### Explanation:\n",
    "- **Missing Values Handling:** \n",
    "    - We checked for missing values in the dataset. Since there were no missing values, no further imputation was required.\n",
    "    \n",
    "- **Train-Test Split:** \n",
    "    - We divided the dataset into **80% training data** and **20% testing data**. \n",
    "    - This ensures that the models are trained on a sufficient amount of data while leaving enough unseen data to evaluate their performance effectively.\n",
    "    \n",
    "- **Feature Scaling:** \n",
    "    - We applied **StandardScaler()** to normalize the feature values. This is necessary because:\n",
    "        - Some machine learning models, such as **SVM** and **k-NN**, are sensitive to the scale of input features. Features with larger numerical values could dominate the learning process if scaling isn't applied.\n",
    "        - Standardization ensures that all features contribute equally to the model, allowing for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3edfd-4e53-416e-9da0-4bfeec832343",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765ae0f3-876c-4f0b-a288-481dc1b50d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "log_reg_accuracy = log_reg.score(X_test, y_test)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Logistic Regression Accuracy: {log_reg_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e33aa-46d3-4e69-950f-a52748ac8ba3",
   "metadata": {},
   "source": [
    "**How it works**: \n",
    "Logistic Regression is a linear model used for binary classification. It predicts the probability of the dependent variable belonging to a particular class (0 or 1). The logistic function (sigmoid function) is used to model the relationship between the input features and the binary target variable. It outputs values between 0 and 1, which can be interpreted as probabilities.\n",
    "\n",
    "**Why it is suitable for this dataset**:\n",
    "- The breast cancer dataset is a binary classification problem (malignant or benign).\n",
    "- Logistic Regression is suitable when the relationship between the dependent variable and independent variables is approximately linear, which makes it an appropriate choice for this dataset.\n",
    "- It is computationally efficient, easy to interpret, and provides probability scores for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc94867-0d7c-43b1-8f43-8845b1b48a8e",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a17240-bf75-450f-8d18-80bd58087e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train the Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "dt_accuracy = dt.score(X_test, y_test)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf875739-336f-43e2-af4e-36fbab4ed878",
   "metadata": {},
   "source": [
    "**How it works**:  \n",
    "A Decision Tree is a tree-like model where each node represents a feature (or attribute), each branch represents a decision rule, and each leaf represents the outcome. The tree is built by recursively splitting the data based on feature values that maximize information gain (reducing entropy) or minimizing impurity (using criteria such as Gini impurity or entropy).\n",
    "\n",
    "**Why it is suitable for this dataset**:\n",
    "\n",
    "- Decision Trees can capture non-linear relationships between features, which makes them suitable for datasets where relationships are more complex.\n",
    "- It is interpretable and easy to visualize, making it easier to understand the model's decision-making process.\n",
    "- It works well with both numerical and categorical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27d24c-cb63-40a9-ac61-58f140844d8c",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23834355-c60a-42e3-9dbc-e92fd2c39343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "rf_accuracy = rf.score(X_test, y_test)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e75764e-3e04-498e-ab18-cb57a58154e5",
   "metadata": {},
   "source": [
    "**How it works**:  \n",
    "A Random Forest is an ensemble of decision trees where each tree is built using a random subset of the features and samples. The final prediction is made by averaging the predictions of all individual trees (for regression) or using the majority vote (for classification).\n",
    "\n",
    "**Why it is suitable for this dataset**:\n",
    "\n",
    "- Random Forest handles high-dimensional data (many features) and works well with large datasets, making it suitable for complex datasets like breast cancer data.\n",
    "- It reduces overfitting by averaging the results of multiple trees, improving generalization.\n",
    "- Random Forest can handle both classification and regression problems and is less sensitive to noise compared to a single decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152105e-d580-4ab9-8e94-7f57b3a0dd68",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49978ab2-e345-4a8b-8e54-3e55964ca48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the Support Vector Machine model\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "svm_accuracy = svm.score(X_test, y_test)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6240e-b033-499e-b4f8-ccb2788bec27",
   "metadata": {},
   "source": [
    "**How it works**:  \n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm that finds the hyperplane that best separates the data into two classes. SVM works by maximizing the margin between the classes and finding the optimal hyperplane that minimizes classification errors. SVM can also be extended to multi-class problems and supports both linear and non-linear decision boundaries through the kernel trick.\n",
    "\n",
    "**Why it is suitable for this dataset**:\n",
    "\n",
    "- SVM is effective for high-dimensional spaces, making it well-suited for datasets with many features, like the breast cancer dataset.\n",
    "- It is powerful for binary classification tasks where a clear margin of separation exists between the classes.\n",
    "- By using kernels, SVM can model non-linear decision boundaries if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6422e-c28b-4aff-8d77-38b8974ab44b",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors (k-NN) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d053022-0a52-4c2e-9e1e-27ca81646df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize and train the k-NN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "knn_accuracy = knn.score(X_test, y_test)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"k-NN Accuracy: {knn_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e464cd-e6a7-41cd-bd92-d1f21bf6b35e",
   "metadata": {},
   "source": [
    "**How it works**:  \n",
    "k-Nearest Neighbors (k-NN) is a simple, instance-based learning algorithm. It classifies a data point based on the majority class among its `k` nearest neighbors in the feature space. It doesn't learn a model explicitly but rather memorizes the training data and makes predictions based on similarity (distance metric) to nearby data points.\n",
    "\n",
    "**Why it is suitable for this dataset**:\n",
    "\n",
    "- k-NN works well when the dataset has clear clusters, and the decision boundary is not necessarily linear.\n",
    "- It is easy to implement and interprets the data based on distances between data points.\n",
    "- It doesn't make strong assumptions about the underlying distribution of data, making it a flexible classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f7e6c9d-a880-4c9f-9ed7-4fe51c61063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Logistic Regression: 0.9824561403508771\n",
      "Decision Tree: 0.9415204678362573\n",
      "Random Forest: 0.9707602339181286\n",
      "SVM: 0.9707602339181286\n",
      "k-NN: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the accuracies of all models\n",
    "model_accuracies = {\n",
    "    \"Logistic Regression\": log_reg_accuracy,\n",
    "    \"Decision Tree\": dt_accuracy,\n",
    "    \"Random Forest\": rf_accuracy,\n",
    "    \"SVM\": svm_accuracy,\n",
    "    \"k-NN\": knn_accuracy\n",
    "}\n",
    "\n",
    "# Print the accuracy of each model\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model, accuracy in model_accuracies.items():\n",
    "    print(f\"{model}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c187d6-d436-4694-ad72-d010176a230a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
